{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae0243b",
   "metadata": {},
   "source": [
    "# Task 4 - CIFAR10 Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3075c",
   "metadata": {},
   "source": [
    "Importing required libraries and set a path for downloading dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50576d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "path_data = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa25a4",
   "metadata": {},
   "source": [
    "In this step we will download the file from internet\n",
    "we can transform data when we are downloading it. The tranformation has 3 parts. first is turning PIL or numpy array data to tensors. second is normalizing data (we have to note that since first we used ToTensor function, the result would be between 0 and 1, so the mean and var for normalization is 0.5 for all channels. after taht we can download data. also we transformed the images in our dataset to one channel. since we don't wantt to try CNN at first place we made this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01336b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "    transforms.Grayscale(num_output_channels=1)])\n",
    "\n",
    "train_set_G = torchvision.datasets.CIFAR10(root=path_data,\n",
    "                                         train=True,\n",
    "                                         download=True,\n",
    "                                         transform= transformation)\n",
    "test_set_G = torchvision.datasets.CIFAR10(root=path_data,\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform= transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc51cf0",
   "metadata": {},
   "source": [
    "In this part with we use a function named DataLoader. this function help us iterating over Itearable datasets. we can also set number of batches for using in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f24db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set_G, batch_size = 5)\n",
    "test_loader = torch.utils.data.DataLoader(test_set_G, batch_size = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366ecb2",
   "metadata": {},
   "source": [
    "we use this function to determine the accuracy of a model over a test dataset. it will count number of correct guesses of our model over total dataset records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62115e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_report(model,dataset):\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataset): \n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "            total += labels.size()[0]\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(\"Accuracy: \", correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b49ff73",
   "metadata": {},
   "source": [
    "## Tuning simple neural network\n",
    "\n",
    "in this step we will create a simple NN and try to find best optimizer and learning rate for it. since the data size is big we would consider just these two factors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79695050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, 200)\n",
    "        self.fc2 = nn.Linear(200, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "     \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 32*32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "M = NeuralNetwork()\n",
    "Loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc85188f",
   "metadata": {},
   "source": [
    "Finding best optimizer for our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83409630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SGD:\n",
      "Accuracy:  0.4159\n",
      "---------------\n",
      "Accuracy for NAdam:\n",
      "Accuracy:  0.4108\n",
      "---------------\n",
      "Accuracy for Adam:\n",
      "Accuracy:  0.4019\n",
      "---------------\n",
      "Accuracy for RMSprop:\n",
      "Accuracy:  0.3864\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "optimizer1 = optim.SGD(M.parameters(), lr=0.001)\n",
    "optimizer2 = optim.NAdam(M.parameters(), lr=0.001)\n",
    "optimizer3 = optim.Adam(M.parameters(), lr=0.001)\n",
    "optimizer4 = optim.RMSprop(M.parameters(), lr=0.001)\n",
    "\n",
    "loss_across_epochs = []\n",
    "num_epochs = 10\n",
    "set_size = 10000\n",
    "optimizers = [optimizer1,optimizer2,optimizer3,optimizer4]\n",
    "opt_labels = ['SGD','NAdam','Adam','RMSprop']\n",
    "    \n",
    "for i in range(4):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss= 0.0\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            optimizers[i].zero_grad()\n",
    "            outputs = M(inputs)  # forward pass \n",
    "            loss = Loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizers[i].step()\n",
    "            train_loss += loss.item()\n",
    "    loss_across_epochs.extend([train_loss/set_size])\n",
    "    print('Accuracy for {}:'.format(opt_labels[i]))\n",
    "    acc_report(M,test_loader)\n",
    "    print('---------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea106b",
   "metadata": {},
   "source": [
    "Finding best learning rate for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51038f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for learning rate 0.1:\n",
      "Accuracy:  0.1\n",
      "---------------\n",
      "Accuracy for learning rate 0.01:\n",
      "Accuracy:  0.1\n",
      "---------------\n",
      "Accuracy for learning rate 0.001:\n",
      "Accuracy:  0.1\n",
      "---------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18700/2343471211.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(M.parameters(), lr=0.001)\n",
    "\n",
    "loss_across_epochs = []\n",
    "num_epochs = 10\n",
    "set_size = 10000\n",
    "lr_list = [0.1,0.01,0.001]\n",
    "    \n",
    "for i in range(3):\n",
    "    optimizer = optim.SGD(M.parameters(), lr=lr_list[i])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss= 0.0\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = M(inputs)  # forward pass \n",
    "            loss = Loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "    loss_across_epochs.extend([train_loss/set_size])\n",
    "    print('Accuracy for learning rate {}:'.format(lr_list[i]))\n",
    "    acc_report(M,test_loader)\n",
    "    print('---------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0a72b4",
   "metadata": {},
   "source": [
    "So the model best fits for SGD optimizer and learning rate of 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942df64",
   "metadata": {},
   "source": [
    "Now we will test on a CNN network with dropouts and Maxpooling\n",
    "we will load the data again, since in this stage we wan t to use all 3 channels of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a0e2ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "train_set_G = torchvision.datasets.CIFAR10(root=path_data,\n",
    "                                         train=True,\n",
    "                                         download=True,\n",
    "                                         transform= transformation)\n",
    "test_set_G = torchvision.datasets.CIFAR10(root=path_data,\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform= transformation)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set_G, batch_size = 5)\n",
    "test_loader = torch.utils.data.DataLoader(test_set_G, batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32085116",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5) \n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "M = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc26806b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SGD:\n",
      "Accuracy:  0.4789\n",
      "---------------\n",
      "Accuracy for NAdam:\n",
      "Accuracy:  0.5617\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "optimizer1 = optim.SGD(M.parameters(), lr=0.001)\n",
    "optimizer2 = optim.NAdam(M.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "loss_across_epochs = []\n",
    "num_epochs = 10\n",
    "set_size = 10000\n",
    "optimizers = [optimizer1,optimizer2]\n",
    "opt_labels = ['SGD','NAdam']\n",
    "    \n",
    "for i in range(2):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss= 0.0\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            optimizers[i].zero_grad()\n",
    "            outputs = M(inputs)  # forward pass \n",
    "            loss = Loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizers[i].step()\n",
    "            train_loss += loss.item()\n",
    "    loss_across_epochs.extend([train_loss/set_size])\n",
    "    print('Accuracy for {}:'.format(opt_labels[i]))\n",
    "    acc_report(M,test_loader)\n",
    "    print('---------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee19239",
   "metadata": {},
   "source": [
    "As we can see, NAdam perforemd better in CNN this time.\n",
    "now we would also try a different structure for our CNN with NAdam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "067176f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(32,64,3) \n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv3 = nn.Conv2d(64,128,3)\n",
    "        self.fc1 = nn.Linear(128*2*2, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 10)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128*2*2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "M = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7798ef42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for new model arch\n",
      "Accuracy:  0.5964\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.NAdam(M.parameters(), lr=0.001)\n",
    "loss_across_epochs = []\n",
    "num_epochs = 10\n",
    "set_size = 10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss= 0.0\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = M(inputs)  # forward pass \n",
    "        loss = Loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "loss_across_epochs.extend([train_loss/set_size])\n",
    "print('Accuracy for new model arch')\n",
    "acc_report(M,test_loader)\n",
    "print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a3891",
   "metadata": {},
   "source": [
    "Since we performed model on our data set using different parameters, we can say simple neural networks performed weaker for our data set and using a CNN resulted in better accuracy for our dataset. Also the second CNN model which has more dropouts and more depth in first Conv2d layer was performing better than the other one. So we can say using these dropouts and also having these depth caused better performance on out model and the model was able to find better patterns in images to separate them from eachother. so, we would recommend this network. for better performance in the future.\n",
    "The final CNN performance on test dataset can be seen below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb66a8d",
   "metadata": {},
   "source": [
    "Also for more accurate result we can use 1000 epochs (since we used 10 epochs for our models to find the best one, and also dataset is very larg to perform this amount of epochs on a shoet time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5417e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model (to be run with a powerful device using cuda to have results in less time)\n",
    "\n",
    "optimizer = optim.NAdam(M.parameters(), lr=0.001)\n",
    "loss_across_epochs = []\n",
    "num_epochs = 1000\n",
    "set_size = 10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss= 0.0\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = M(inputs)  # forward pass \n",
    "        loss = Loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "loss_across_epochs.extend([train_loss/set_size])\n",
    "print('Accuracy for Final Model')\n",
    "acc_report(M,test_loader)\n",
    "print('---------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
